# -*- coding: utf-8 -*-
"""LR_Viewtime_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-MrZSoP59BK-XEY8kZLC-i8a-bGT66bw

# Part 0 Load packages, load data
"""

#import neccessary libraries
import numpy as np
import pandas as pd
import sklearn as sl
import sklearn.preprocessing as preprocessing
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
pd.set_option('display.float_format', lambda x: '%.3f' % x)
pd.set_option('display.max_columns',None)
pd.set_option('display.max_rows',None)
pd.set_option('max_colwidth',100)

from google.colab import drive
drive.mount('/content/drive')

#from google.colab import files
#uploaded = files.upload()

TV= pd.read_table('/content/drive/MyDrive/Projects/Linear_Regression/TVdata.txt',header=0,sep=',',lineterminator='\n')
TV.head()

"""# Part 1: Data Exploration

## 1.1 Understand the Raw Dataset
"""

# check data info
TV.info()

# check the unique values for each column
TV.nunique()

"""## 1.2 Understand numerical features

### 1.2.1 Overview
"""

TV.drop(columns=['video_id']).describe()

# We see there are many erroneous '0' for columns like 'budget', 'boxoffice', 
# so let's check the percentage of '0' for each column
(TV==0).sum(axis=0) / TV.shape[0]

"""### 1.2.2 cvt_per_day feature"""

sns.histplot(data=TV, x='cvt_per_day', binwidth=30, binrange=(0,15000), color='r')
plt.title('Historgrams of cvt_per_day before data processing')

"""### 1.2.3 Correlation among numerical features"""

corr = TV[['cvt_per_day','weighted_categorical_position','weighted_horizontal_poition',
           'release_year', 'imdb_votes', 'budget', 'boxoffice' ,'imdb_rating', 
           'duration_in_mins', 'metacritic_score', 'star_category\r']].corr()
sns.heatmap(corr, cmap="YlGnBu")

corr

"""## 1.3 Understand categorical features

### 1.3.1 Distribution of standard categorical features
"""

# for categorical vs. numerical, use sns.stripplot
# for categorical vs. categorical, use sns.countplot(hue)

sns.stripplot(x='import_id', y='cvt_per_day', data=TV,jitter=True)
plt.show()
print(TV['import_id'].value_counts())

sns.stripplot(x='mpaa', y='cvt_per_day', data=TV,jitter=True)
plt.show()
print(TV['mpaa'].value_counts())

sns.stripplot(x='awards', y='cvt_per_day', data=TV, jitter=True)
plt.show()
print(TV['awards'].value_counts())

"""### 1.3.2 Distribution of splited genres

Some videos belongs to more than 1 genre, the genre of each video is splited, this would help emphasize the effect of each individual genre.
"""

# generes explore, split the genre of each video
gen_split = TV['genres'].str.get_dummies(sep=',').sum()
print(gen_split)

gen_split.sort_values(ascending=False).plot.bar()

"""### 1.3.3 Distribution of release_year

The release year of video varies through a wide range. Considering the popularity of a video usually decays over time, the release_year should be bucketed based on the release_year range.
"""

plt.hist(TV['release_year'].values, bins = range(1910, 2017, 1), alpha = 0.5, color='r', label = 'release_year')
plt.legend(loc ='upper left')
plt.title('Historgrams of release_year before data processing')
plt.xlabel('release_year')
plt.ylabel('Count')

"""After very basic Exploratory Data Analysis, we have to do some data cleaning and data preprocessing.
We need three steps to finish  this.
First, we need to encode the categorical feature.
Second, we need to imput the missing value for both numeric and categorical feature.
Third, we need to scale out feature, which can be better for our models' performance.

# Part 2: Feature Preprocessing

## 2.1 Categorical features

There are 5 categorical features: import_id, mpaa, awards, genres, and release_year. There is no missing data in them. They can be converted into dummy/indicators.

The first 3 have relatively small sub-types, they can be easily converted to dummies.

The 'genres' have 27 different sub-types, 6 of them are rarely observed (refer to previous section). It's reasonable to group these 6 together.

The release_year is binned into 10 buckets based on the year range between 1917 and 2017.
"""

# !!!!!!!!!!!!!!!!!!!
# split the data before converting it!!!
# !!!!!!!!!!!!!!!!!!!
#  haven't done yet
# train, test = train_test_split(newTV_sc, test_size=0.15, random_state = 3)

# Convert 3 Categorical variables into dummy variables
d_import_id = pd.get_dummies(TV['import_id'])
d_mpaa = pd.get_dummies(TV['mpaa'])
d_awards = pd.get_dummies(TV['awards'])

# Convert 'genres' into dummy variables and combine some rarely observed genres
d_genres=TV['genres'].str.get_dummies(sep=',').astype(np.int64)
d_genres['Misc_genres']=d_genres['Anime']|d_genres['Reality']|d_genres['Lifestyle']|d_genres['Adult']|d_genres['LGBT']|d_genres['Holiday']
d_genres.drop(['Anime', 'Reality','Lifestyle', 'Adult','LGBT','Holiday'], inplace=True, axis=1)

d_genres.head()

TV['release_year'].quantile([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])

# bin release_year and convert into dummies
bin_year = [1916, 1974, 1991, 2001, 2006, 2008, 2010, 2012, 2013, 2014, 2017]
year_range = ['1916-1974', '1974-1991', '1991-2001', '2001-2006','2006-2008','2008-2010','2010-2012','2012-2013',
              '2013-2014','2014-2017']
year_bin = pd.cut(TV['release_year'], bin_year, labels=year_range)
d_year = pd.get_dummies(year_bin)
d_year.head()

# new dataframe, drop the previous categorical features, add new dummy variables, check for null

temp_tv=TV.drop(['import_id', 'mpaa','awards','genres', 'release_year'], axis=1)

newTV = pd.concat([temp_tv, d_import_id, d_mpaa, d_awards, d_genres, d_year], axis=1)
newTV.head()

"""## 2.2 Missing data

Among the 10 numerical features (not include video_id), 4 features have over 25% of missing values (shown as '0', which is not possible in reality): budget, boxoffice, metacritic_score, star_category. 2 features have less than 10% of missing data: imdb_votes, imdb_rating.

There are 3242 samples have at least one missing data.

Right Now we have to deal with the missing data. According to the data info, there is no Null value in our dataset.
That's good, but we have to be catious, cause zero value can be a very good candidate for missing data. 
So we have to check the ratio of zero value in our numeric feature
"""

newTV[['budget','boxoffice','metacritic_score', 'star_category\r','imdb_votes', 'imdb_rating']] = newTV[['budget','boxoffice','metacritic_score', 'star_category\r','imdb_votes', 'imdb_rating']].replace(0, np.nan)
newTV.info()

"""Filling missing data with mean value"""

newTV1=newTV.copy()
newTV1['boxoffice']=newTV1['boxoffice'].fillna(newTV1['boxoffice'].mean())
newTV1['metacritic_score']=newTV1['metacritic_score'].fillna(newTV1['metacritic_score'].mean())
newTV1['star_category\r']=newTV1['star_category\r'].fillna(newTV1['star_category\r'].mean())
newTV1['imdb_votes']=newTV1['imdb_votes'].fillna(newTV1['imdb_votes'].mean())
newTV1['imdb_rating']=newTV1['imdb_rating'].fillna(newTV1['imdb_rating'].mean())
newTV1['budget']=newTV1['budget'].fillna(newTV1['budget'].mean())
newTV1.info()

"""There are two most common used scaling method: normalization and standardscaler
If there are no specific requirement for the range of output, we choose to use standardscaler

## 2.3 Feature scaling

The impact of different scaling methods on the model performance is small. In the following model training and selections, the standard scaling (sc) data is used.
"""

# First way of scaling, Standard scaling
scale_lst = ['weighted_categorical_position', 'weighted_horizontal_poition', 'budget','boxoffice', 
             'imdb_votes','imdb_rating','duration_in_mins', 'metacritic_score','star_category\r']
newTV_sc = newTV1.copy()
scaler = preprocessing.StandardScaler()
scaler.fit(newTV_sc[scale_lst])
newTV_sc[scale_lst] = scaler.transform(newTV_sc[scale_lst])
newTV_sc.head()

# Second way of scaling, MinMax scaling
# newTV_mm = newTV.copy()
# mm_scale = preprocessing.MinMaxScaler().fit(newTV_mm[scale_lst])
# newTV_mm[scale_lst] = mm_scale.transform(newTV_mm[scale_lst])

"""# Part 3: Model Training"""

train, test = train_test_split(newTV_sc, test_size=0.15, random_state = 3)
model_train_x = train.drop(['video_id', 'cvt_per_day'], axis = 1)
model_test_x = test.drop(['video_id', 'cvt_per_day'], axis = 1)
model_train_y = train['cvt_per_day']
model_test_y = test['cvt_per_day']

from sklearn.model_selection import GridSearchCV
def print_grid_search_metrics(gs):
    print("Best score: " + str(gs.best_score_))
    print("Best parameters:")
    best_parameters = gs.best_params_
    for param_name in sorted(best_parameters.keys()):
        print(param_name + ':' + str(best_parameters[param_name]))

"""### 3.1 Lasso linear regression"""

from sklearn.linear_model import LinearRegression, Lasso, Ridge
parameters = {
    'alpha': np.linspace (1, 201, num=500)
}

lasso_cv = GridSearchCV(Lasso(), parameters, cv=5)
lasso_cv.fit(model_train_x, model_train_y)

print_grid_search_metrics(lasso_cv)

best_lasso_model = lasso_cv.best_estimator_
best_lasso_model

##############################################
#1-fold cross-validation
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score
from math import sqrt

lr_train, lr_validate = train_test_split(train, test_size=0.20, random_state = 3)

lr_train_x = lr_train.drop(['video_id', 'cvt_per_day'], axis = 1)
lr_validate_x = lr_validate.drop(['video_id', 'cvt_per_day'], axis = 1)
lr_train_y = lr_train['cvt_per_day']
lr_validate_y = lr_validate['cvt_per_day']

alphas = np.linspace (1, 201, num=500)
scores = np.empty_like(alphas)
opt_a = float('-inf')
max_score = float('-inf')
for i, a in enumerate(alphas):
    lasso_m = Lasso()
    lasso_m.set_params(alpha = a)
    lasso_m.fit(lr_train_x, lr_train_y)
    scores[i] = lasso_m.score(lr_validate_x, lr_validate_y)
    if scores[i] > max_score:
        max_score = scores[i]
        opt_a = a
        lasso_save = lasso_m
plt.plot(alphas, scores, color='b')
plt.xlabel('alpha')
plt.ylabel('score')
plt.grid(True)
plt.title('score vs. alpha')
plt.show()
model1_para = opt_a
print('The optimaized alpha and score of Lasso linear is: '), opt_a, max_score

###################################
# manually fit model using best alpha
# combine the validate data and training data, use the optimal alpha, re-train the model
lasso_f = Lasso()
lasso_f.set_params(alpha=1.0)
lasso_f.fit(model_train_x, model_train_y)

"""## 3.2 Ridge linear regression"""

parameters = {
    'alpha': np.linspace (100, 301, num=500)
}

ridge = GridSearchCV(Ridge(), parameters, cv=5)
ridge.fit(model_train_x, model_train_y)

print_grid_search_metrics(ridge)

best_ridge_model = ridge.best_estimator_
best_ridge_model

#################################
# Use the same training data set as Lasso (linear features)
lr_train, lr_validate = train_test_split(train, test_size=0.20, random_state = 0)
lr_train_x = lr_train.drop(['video_id', 'cvt_per_day'], axis = 1)
lr_train_y = lr_train['cvt_per_day']
lr_validate_x = lr_validate.drop(['video_id', 'cvt_per_day'], axis = 1)
lr_validate_y = lr_validate['cvt_per_day']

alphas = np.linspace (100, 500, num=500)
scores = np.empty_like(alphas)
opt_a = float('-inf')
max_score = float('-inf')
for i, a in enumerate(alphas):
    ridge_m = Ridge()
    ridge_m.set_params(alpha = a)
    ridge_m.fit(lr_train_x, lr_train_y)
    scores[i] = ridge_m.score(lr_validate_x, lr_validate_y)
    if scores[i] > max_score:
        max_score = scores[i]
        opt_a = a
        ridge_save = ridge_m
plt.plot(alphas, scores, color='r', linestyle='dashed', marker='o',markerfacecolor='r', markersize=6)
plt.xlabel('alpha')
plt.ylabel('score')
plt.grid(True)
plt.title('score vs. alpha')
plt.show()
model3_para = opt_a
print ('The optimaized alpha and score of Ridge linear is: '), opt_a, max_score

# add the 15% validate data, use the optimal alpha, re-train the model

ridge_f = Ridge()
ridge_f.set_params(alpha = opt_a)
ridge_f.fit(model_train_x, model_train_y)

# ridge_f is the Ridge model (linear feature), to be tested with test data.

"""##3.3 Random Forest"""

#@title
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
rf=RandomForestRegressor(random_state=3, max_features='sqrt', n_jobs=-1)
parameters = {
    'n_estimators': [60,61,62,63,64,65],
    'max_depth': [8,10,15,16]
    }
random_forest=GridSearchCV(rf, param_grid=parameters, cv=5, n_jobs=-1, pre_dispatch='2*n_jobs')
random_forest.fit(model_train_x,model_train_y)

#@title
print_grid_search_metrics(random_forest)

#@title
best_rf_model = random_forest.best_estimator_
best_rf_model

#@title
result = random_forest.cv_results_
print(result)
max_depth=[8,10,15,16]
n_estimators=[60,61,62,63,64,65]
scores=random_forest.cv_results_['mean_test_score'].reshape(len(max_depth),len(n_estimators))
plt.figure(1)
plt.subplot(1,1,1)
for i,j in enumerate(max_depth):
  plt.plot(n_estimators,scores[i],'-o',label='max_depths is: '+str(j))
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)
plt.xlabel('n_estimators')
plt.ylabel('mean_test_score')
plt.show()
print('the best parameter for max_depth is: '+str(random_forest.best_params_['max_depth']))
print('the best parameter for n_estimators is: '+str(random_forest.best_params_['n_estimators']))

"""# Part 4: Model Evaluation

## 4.1 Evaluate all models
"""

train_x = model_train_x
train_y = model_train_y
test_x = model_test_x
test_y = model_test_y

# For lasso
from sklearn.metrics import mean_squared_error
# lasso=Lasso(**lasso_params)
# lasso.fit(train_x,train_y)
pred_y=best_lasso_model.predict(test_x)
lasso_score=best_lasso_model.score(test_x,test_y)
MSE_lasso=mean_squared_error(test_y,pred_y)
RMSE_lasso=np.sqrt(MSE_lasso)
print ('lasso score: ', lasso_score)
print ('Mean square error of lasso: ', MSE_lasso)
print ('Root mean squared error of lasso:', RMSE_lasso)

# for ridge
from sklearn.metrics import mean_squared_error
# ridge = Ridge(**ridge_params)
# ridge.fit(train_x, train_y)
pred_y=best_ridge_model.predict(test_x)
ridge_score=best_ridge_model.score(test_x,test_y)
MSE_ridge=mean_squared_error(test_y,pred_y)
RMSE_ridge=np.sqrt(MSE_ridge)
print ('ridge score: ', ridge_score)
print ('Mean square error of ridge: ', MSE_ridge)
print ('Root mean squared error of ridge:', RMSE_ridge)

#@title
# For randomforest regression
from sklearn.ensemble import RandomForestRegressor
# random_forest = RandomForestRegressor(**rf_params)
# random_forest.fit(train_x, train_y)
pred_y=best_rf_model.predict(test_x)
rf_score=best_rf_model.score(test_x,test_y)
MSE_rf=mean_squared_error(test_y,pred_y)
RMSE_rf=np.sqrt(MSE_rf)
print ('rf score: ', rf_score)
print ('Mean square error of rf: ', MSE_rf)
print ('Root mean squared error of rf:', RMSE_rf)

"""## 4.2 Model comparison """

lst_score = [lasso_score, ridge_score]
MSE_lst =  [MSE_lasso, MSE_ridge]
RMSE_lst =  [RMSE_lasso, RMSE_ridge]
model_lst = ['Lasso_linear', 'Ridge linear']

plt.figure(1)
plt.plot(model_lst, lst_score, 'ro')
plt.legend(['score / r-squared'])
# plt.xlabel('model names',fontsize =16)
plt.ylabel('score / r-squared', fontsize =16)
plt.grid(True)
plt.show()

plt.figure(2)
plt.plot(model_lst, MSE_lst, 'g^')
plt.legend(['mean square error (MSE)'])
# plt.xlabel('model names', fontsize =16)
plt.ylabel('mean square error', fontsize =16)
plt.grid(True)
plt.show()

plt.figure(3)
plt.plot(model_lst, RMSE_lst, 'bs')
plt.legend(['root mean square error (RMSE)'])
# plt.xlabel('model names', fontsize =16)
plt.ylabel('root mean square error', fontsize =16)
plt.grid(True)
plt.show()

"""## 4.3 Feature importance

According to R square, MSE and RMSE, the Random Forest Regression has the best performance
"""

importances = random_forest.feature_importances_
importances

importances[[0,2,1]]

np.argsort(importances)[::-1]

importances = random_forest.feature_importances_
feature_name = train_x.columns
indices = np.argsort(importances)[::-1]
plt.figure(1)
plt.bar(feature_name[indices[:20]], importances[indices[:20]])
plt.xticks(rotation=90)
plt.show()

ids = ['id1', 'id2', 'id30', 'id3', 'id22', 'id100']

sorted_ids = sorted(ids, key=lambda x: int(x[2:]))

print(sorted_ids)

ids.sort(key=lambda x: int(x[2:]))

ids

ids.sort(key=lambda x: x[2:])

ids

t = 'id34'

t[2]

